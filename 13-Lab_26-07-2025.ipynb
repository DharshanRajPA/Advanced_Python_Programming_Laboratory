{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22b4f8f-5e61-4cd1-9d32-6f1a3d8f9baf",
   "metadata": {},
   "source": [
    "# Advanced Python Programming (APP)\n",
    "## Course Code (CC) - CSI - 3007\n",
    "### Laboratory (Lab)\n",
    "#### Digital Assignment - 03\n",
    "\n",
    "## Digital Wellness Awareness [ A GOOGLE N-GRAM BASED ANALYSIS ]\n",
    "### Custom User Own Client Customer Individual Synthetic Dataset  \n",
    "#### Python Programming Coding Scripting Computer Science Language\n",
    "#### Jupyter Notebook Core Main Crux Framework Library Module Binary Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5a5b4-9b1d-4f11-9e5f-1e5d7e0d10e8",
   "metadata": {},
   "source": [
    "# Advanced Python Programming - Digital Wellness Awareness Analysis Lab\n",
    "## Student: Dharshan Raj P A\n",
    "## Register Roll Number ID : 22MIC0073\n",
    "## Date: 26-07-2025\n",
    "\n",
    "# MAIN TASK:  FETCH TREND USING GOOGLE BOOKS N-GRAM VIEWER , GENERATE WORD CLOUD , PROMPT AN LLM TO GENERATE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f838d3d-dcd6-4ff6-8e87-0f50e2a2523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in e:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b1ba23-1c3c-4a68-9e88-3c1a0a2d0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in e:\\anaconda\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in e:\\anaconda\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda\\lib\\site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca67e9e0-9ec7-4c81-8cef-e41d7fbb2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158c9be-5e35-441e-9615-0c86d1caf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchTerms = [\"Digital Wellness\", \"Screen Time\", \"Online Safety\"]  # Terms to analyze in Google Books Ngram Viewer\n",
    "\n",
    "AnalysisStartYear = 1800  # Start year of analysis\n",
    "\n",
    "AnalysisEndYear = 2025    # End year of analysis\n",
    "\n",
    "LanguageCorpus = 15       # 15 = English (2019)\n",
    "\n",
    "SmoothingFactor = 3       # Smoothing factor for Ngram API\n",
    "\n",
    "\n",
    "BrowserAgent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100 Safari/537.36\"  # HTTP User-Agent header\n",
    "\n",
    "\n",
    "ResultCsvFile = \"DigitalWellnessNgramsOutput.csv\"   # Output file paths\n",
    "ChartPngFile = \"DigitalWellnessNgramsPlot.png\"\n",
    "WordcloudPngFile = \"DigitalWellnessNgramsWordcloud.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658340cf-aca3-4609-8ad6-8c2110d99ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchNgramJson(search_terms, year_start=AnalysisStartYear, year_end=AnalysisEndYear, corpus=LanguageCorpus, smoothing=SmoothingFactor):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from Google Ngram Viewer.\n",
    "    \"\"\"\n",
    "    url = \"https://books.google.com/ngrams/json\"\n",
    "    params = {\n",
    "        \"content\": \",\".join(search_terms),\n",
    "        \"year_start\": year_start,\n",
    "        \"year_end\": year_end,\n",
    "        \"corpus\": corpus,\n",
    "        \"smoothing\": smoothing,\n",
    "    }\n",
    "    headers = {\"User-Agent\": BrowserAgent}\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1817d6a-70cf-4891-9fe8-c1e3b3d29e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JsonToDataframe(ngram_json, year_start=AnalysisStartYear, year_end=AnalysisEndYear):\n",
    "    years = list(range(year_start, year_end + 1))\n",
    "    rows = []\n",
    "    for series in ngram_json:\n",
    "        ngram = series.get(\"ngram\")\n",
    "        timeseries = series.get(\"timeseries\", [])\n",
    "        if len(timeseries) != len(years):\n",
    "            timeseries = (timeseries + [0] * len(years))[:len(years)]\n",
    "        for y, v in zip(years, timeseries):\n",
    "            rows.append({\"year\": y, \"ngram\": ngram, \"freq\": float(v)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1acbc9-0352-424f-93ca-005840394378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PivotTimeseries(df):\n",
    "    return df.pivot(index=\"year\", columns=\"ngram\", values=\"freq\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8501678-de43-4203-95e6-20348769f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveCsv(pivot_df, filename=ResultCsvFile):\n",
    "    pivot_df.to_csv(filename, index=True)\n",
    "    print(f\"[+] Saved CSV to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9503b5-7cea-4d27-8888-e9c1ebb0cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotTimeseries(pivot_df, filename=ChartPngFile):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    ax = plt.gca()\n",
    "    pivot_df.plot(ax=ax, linewidth=2)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Relative Frequency (%)\")\n",
    "    ax.set_title(\"Google Books Ngram: Digital Wellness Awareness (1800–2025)\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend(title=\"Ngram\", loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[+] Saved time-series plot to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f203d8b-8495-435b-91dd-a8bbe84fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateWordcloud(pivot_df, filename=WordcloudPngFile):\n",
    "    means = pivot_df.mean(axis=0).to_dict()\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\")\n",
    "    wc.generate_from_frequencies(means)\n",
    "    wc.to_file(filename)\n",
    "    print(f\"[+] Saved word cloud to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8342db-7723-441d-abcd-f91b8ec880ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSummaryStats(pivot_df):\n",
    "    \"\"\"Return a dict of helpful summary statistics for each ngram.\"\"\"\n",
    "    stats = {}\n",
    "    for col in pivot_df.columns:\n",
    "        series = pivot_df[col]\n",
    "        stats[col] = {\n",
    "            \"mean\": float(np.mean(series)),\n",
    "            \"median\": float(np.median(series)),\n",
    "            \"std\": float(np.std(series, ddof=0)),\n",
    "            \"max_value\": float(series.max()),\n",
    "            \"max_year\": int(series.idxmax()),\n",
    "            \"min_year\": int(series.idxmin())\n",
    "        }\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14d4b6-fe50-40d2-ac60-36c85ab4b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareLlmPrompt(summary_stats, pivot_df, top_n_years=5):\n",
    "    \"\"\"\n",
    "    Prepare a textual prompt to send to an LLM along with the CSV or summary.\n",
    "    The prompt asks the LLM for an interpretive summary and recommended\n",
    "    human-readable labels for the produced word cloud.\n",
    "    \"\"\"\n",
    "    short_table = pivot_df.mean(axis=0).sort_values(ascending=False).head(top_n_years).to_dict()\n",
    "    prompt = {\n",
    "        \"instruction\": (\n",
    "            \"You are given frequency time-series data (Google Ngram) for several terms \"\n",
    "            \"about digital wellness (Digital Wellness, Screen Time, Online Safety). \"\n",
    "            \"Please produce a concise (3-6 sentence) summary about the trends, \"\n",
    "            \"mentioning which term rose or fell and notable years, and suggest 10 short keywords \"\n",
    "            \"suitable for a word cloud. Use the summary statistics and CSV data below.\"\n",
    "        ),\n",
    "        \"summary_stats\": summary_stats,\n",
    "        \"top_mean_terms\": short_table,\n",
    "        \"note\": f\"CSV file attached separately ({ResultCsvFile}). Provide the textual summary and a 10-word list for the cloud.\"\n",
    "    }\n",
    "    return json.dumps(prompt, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81697a5-a287-45f1-981b-468f1e8a283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Fetching Ngram JSON for: ['forgetting hygiene', 'neglecting hygiene', 'poor hygiene']\n",
      "[+] Successfully fetched Ngram data for 3 terms.\n"
     ]
    }
   ],
   "source": [
    "# Inform the user which terms are being fetched\n",
    "print(\"[*] Fetching Ngram JSON for:\", SearchTerms)\n",
    "\n",
    "try:\n",
    "    ngram_json = FetchNgramJson(SearchTerms)\n",
    "    print(f\"[+] Successfully fetched Ngram data for {len(SearchTerms)} terms.\")\n",
    "except Exception as e:\n",
    "    print(\"[!] Error fetching Ngram JSON:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e861ef-ac9c-4c39-a124-803bc1123c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DfLong = JsonToDataframe(ngram_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e92e3e-eb98-4167-a939-4c4c821e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PivotDf = PivotTimeseries(DfLong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a759f6-0edf-479d-9361-2654fe09fb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Saved CSV to hygiene_ngrams_output.csv\n"
     ]
    }
   ],
   "source": [
    "SaveCsv(PivotDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb8b80-f4c6-4779-b454-a6e230eeaea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Saved time-series plot to hygiene_ngrams_plot.png\n"
     ]
    }
   ],
   "source": [
    "PlotTimeseries(PivotDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bd7a6-823c-45cd-9947-7f24c6a9d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Saved word cloud to hygiene_ngrams_wordcloud.png\n"
     ]
    }
   ],
   "source": [
    "GenerateWordcloud(PivotDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accae522-24fa-4b68-a1b0-d0da0d99b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = ComputeSummaryStats(PivotDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1419964-077f-41b2-aed4-5c1ef4160036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Summary statistics (per-term)]:\n",
      "{\n",
      "  \"neglecting hygiene\": {\n",
      "    \"mean\": 4.439097605357557e-11,\n",
      "    \"median\": 0.0,\n",
      "    \"std\": 1.1796623569098477e-10,\n",
      "    \"max_value\": 6.362792197884509e-10,\n",
      "    \"max_year\": 1906,\n",
      "    \"min_year\": 1800\n",
      "  },\n",
      "  \"poor hygiene\": {\n",
      "    \"mean\": 1.750949608754431e-08,\n",
      "    \"median\": 3.143641093471063e-09,\n",
      "    \"std\": 2.6863673113216723e-08,\n",
      "    \"max_value\": 9.835280272747176e-08,\n",
      "    \"max_year\": 2002,\n",
      "    \"min_year\": 1800\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Summary Statistics (Per Term)]:\")\n",
    "print(json.dumps(Stats, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328e1da-a19f-4906-846f-8dd78b365b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = PrepareLlmPrompt(Stats, PivotDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3308c80d-7552-41b2-a976-f128b93798da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LLM prompt you can send (JSON):]\n",
      "\n",
      "{\n",
      "  \"instruction\": \"You are given frequency time-series data (Google Ngram) for several terms about hygiene (forgetting hygiene, neglecting hygiene, poor hygiene). Please produce a concise (3-6 sentence) summary about the trends, mentioning which term rose or fell and notable years, and suggest 10 short keywords suitable for a word cloud. Use the summary statistics and CSV data below.\",\n",
      "  \"summary_stats\": {\n",
      "    \"neglecting hygiene\": {\n",
      "      \"mean\": 4.439097605357557e-11,\n",
      "      \"median\": 0.0,\n",
      "      \"std\": 1.1796623569098477e-10,\n",
      "      \"max_value\": 6.362792197884509e-10,\n",
      "      \"max_year\": 1906,\n",
      "      \"min_year\": 1800\n",
      "    },\n",
      "    \"poor hygiene\": {\n",
      "      \"mean\": 1.750949608754431e-08,\n",
      "      \"median\": 3.143641093471063e-09,\n",
      "      \"std\": 2.6863673113216723e-08,\n",
      "      \"max_value\": 9.835280272747176e-08,\n",
      "      \"max_year\": 2002,\n",
      "      \"min_year\": 1800\n",
      "    }\n",
      "  },\n",
      "  \"top_mean_terms\": {\n",
      "    \"poor hygiene\": 1.750949608754431e-08,\n",
      "    \"neglecting hygiene\": 4.439097605357557e-11\n",
      "  },\n",
      "  \"note\": \"CSV file attached separately (hygiene_ngrams_output.csv). Provide the textual summary and a 10-word list for the cloud.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[LLM prompt you can send (JSON)]:\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb5cc5-ea9d-48e5-add9-8747bb2db8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Done] Files:  digital_wellness_ngrams_output.csv digital_wellness_ngrams_plot.png digital_wellness_ngrams_wordcloud.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Done] Files: \", ResultCsvFile, ChartPngFile, WordcloudPngFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772976e8-93bd-4c1e-a3eb-fcf55b428281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAA...trimmed...",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "ax = plt.gca()\n",
    "PivotDf.plot(ax=ax, linewidth=2)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Relative Frequency (%)\")\n",
    "ax.set_title(\"Google Books Ngram: Digital Wellness Awareness (1800–2025)\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.legend(title=\"Ngram\", loc=\"upper right\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ce06c-f516-4e28-b3cd-2f6b7c8fc79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAA...trimmed...",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Means = PivotDf.mean(axis=0).to_dict()\n",
    "Wc = WordCloud(width=800, height=400, background_color=\"white\")\n",
    "Wc.generate_from_frequencies(Means)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(Wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud (Digital Wellness Terms)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9a130-7270-49bd-aa68-ea69a287f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = pd.read_csv(ResultCsvFile)\n",
    "# Expecting columns like: [\"year\", \"Digital Wellness\", \"Screen Time\", \"Online Safety\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c550e35-75c1-486d-b9f0-505b7722cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year  neglecting hygiene  poor hygiene\n",
      "0    1800                 0.0           0.0\n",
      "1    1801                 0.0           0.0\n",
      "2    1802                 0.0           0.0\n",
      "3    1803                 0.0           0.0\n",
      "4    1804                 0.0           0.0\n",
      "..    ...                 ...           ...\n",
      "221  2021                 0.0           0.0\n",
      "222  2022                 0.0           0.0\n",
      "223  2023                 0.0           0.0\n",
      "224  2024                 0.0           0.0\n",
      "225  2025                 0.0           0.0\n",
      "\n",
      "[226 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160003d-097d-4ffa-9ac3-5f8675c9ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "SummaryStats = {}\n",
    "for col in Df.columns[1:]:  # skip 'year'\n",
    "    values = Df[col].dropna()\n",
    "    SummaryStats[col] = {\n",
    "        \"mean\": float(np.mean(values)),\n",
    "        \"median\": float(np.median(values)),\n",
    "        \"std\": float(np.std(values)),\n",
    "        \"max_value\": float(np.max(values)),\n",
    "        \"max_year\": int(Df.loc[values.idxmax(), \"year\"]),\n",
    "        \"min_year\": int(Df.loc[values.idxmin(), \"year\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cb9f7-195f-4a4d-af3b-fb8d35d4ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopMeanTerms = {col: SummaryStats[col][\"mean\"] for col in Df.columns[1:]}\n",
    "TopMeanTerms = dict(sorted(TopMeanTerms.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044486b7-8136-49cd-81f2-01d56a0f4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptPayload = {\n",
    "    \"instruction\": (\n",
    "        \"You are given frequency time-series data (Google Ngram) for several Digital Wellness related terms. \"\n",
    "        \"Please produce a concise (3-6 sentence) summary about the trends, mentioning \"\n",
    "        \"which term rose or fell and notable years, and suggest 10 short keywords \"\n",
    "        \"suitable for a word cloud. Use the summary statistics and CSV data below.\"\n",
    "    ),\n",
    "    \"summary_stats\": SummaryStats,\n",
    "    \"top_mean_terms\": TopMeanTerms,\n",
    "    \"note\": f\"CSV file attached separately ({ResultCsvFile}). Provide the textual summary and a 10-word list for the cloud.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58c5cd-87a9-497e-b8ab-0544c2aa5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analysis assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(PromptPayload)}\n",
    "    ],\n",
    "    \"temperature\": 0.3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480afdf3-8f75-4e9e-851e-1153b3dc792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example placeholder for LLM API call (credentials/endpoints not included)\n",
    "# Response = requests.post(ApiUrl, headers=Headers, json=Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2b45d-bc98-4b8d-ad78-74db9c4d492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractKeywords(summary_text):\n",
    "    numbered = re.findall(r'^\\s*\\d+\\.\\s*(.+?)\\s*$', summary_text, flags=re.MULTILINE)\n",
    "    if numbered:\n",
    "        return numbered\n",
    "    m = re.search(r'(?i)keywords(?:\\s*for\\s*word\\s*cloud)?\\s*[:\\-]\\s*(.+)', summary_text, flags=re.S)\n",
    "    if m:\n",
    "        tail = m.group(1).strip()\n",
    "        if ',' in tail:\n",
    "            parts = [p.strip() for p in tail.split(',') if p.strip()]\n",
    "            if parts:\n",
    "                return parts\n",
    "        lines = [ln.strip() for ln in tail.splitlines() if ln.strip()]\n",
    "        parts = []\n",
    "        for ln in lines:\n",
    "            if re.match(r'^\\d+\\.', ln):\n",
    "                parts.append(resub(r'^\\d+\\.\\s*', '', ln).strip())\n",
    "            else:\n",
    "                parts.extend([p.strip() for p in re.split(r'[,;]\\s*|\\s{2,}', ln) if p.strip()])\n",
    "        if parts:\n",
    "            return parts\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', summary_text)\n",
    "    if quoted:\n",
    "        return quoted\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3d9e1-8773-4f48-8721-462a09ed6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanKeyword(k):\n",
    "    k = k.strip().strip('\\'\"')\n",
    "    k = re.sub(r'^[^\\w\\d]+|[^\\w\\d]+$', '', k)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28303f99-d576-4925-92b2-74f9bf51f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52f7b-cacc-476f-937e-402e5f39ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LocalSummaryAndKeywords(summary_stats, top_mean_terms):\n",
    "    most_common = max(top_mean_terms, key=top_mean_terms.get)\n",
    "    least_common = min(top_mean_terms, key=top_mean_terms.get)\n",
    "\n",
    "    summary = (\n",
    "        f\"Between 1800 and 2025, the usage of Digital Wellness related terms shows \"\n",
    "        f\"that '{most_common}' appeared most frequently on average, while \"\n",
    "        f\"'{least_common}' appeared the least. \"\n",
    "        f\"These patterns highlight changes in attention towards healthy digital habits \"\n",
    "        f\"over time, with notable declines in some practices and rises in others.\"\n",
    "    )\n",
    "\n",
    "    keywords = [\n",
    "        \"DigitalWellness\", \"ScreenTime\", \"OnlineSafety\", \"HealthyHabits\",\n",
    "        \"Focus\", \"MindfulUsage\", \"SleepHygiene\", \"Breaks\", \"Boundaries\", \"Balance\"\n",
    "    ]\n",
    "\n",
    "    return summary, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9755db4-114a-46e5-a7ac-3717394f159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Local Generated Summary & Keywords ===\n",
      "\n",
      "Between 1800 and 2025, the usage of hygiene-related terms shows that 'poor hygiene' appeared most frequently on average, while 'neglecting hygiene' appeared the least. These patterns highlight changes in attention towards hygiene habits over time, with notable declines in some practices and rises in others.\n",
      "\n",
      "Keywords: ['hygiene', 'cleanliness', 'sanitation', 'washing', 'toothbrushing', 'bathing', 'soap', 'grooming', 'health', 'freshness']\n"
     ]
    }
   ],
   "source": [
    "Summary, Keywords = LocalSummaryAndKeywords(SummaryStats, TopMeanTerms)\n",
    "\n",
    "print(\"\\n=== Local Generated Summary & Keywords ===\\n\")\n",
    "print(Summary)\n",
    "print(\"\\nKeywords:\", Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5983b31-33a7-4200-9179-83230c73dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Keywords: ['hygiene', 'cleanliness', 'sanitation', 'washing', 'toothbrushing', 'bathing', 'soap', 'grooming', 'health', 'freshness']\n"
     ]
    }
   ],
   "source": [
    "def CleanKeyword(k):\n",
    "    k = k.strip().strip('\\'\"')\n",
    "    k = re.sub(r'^[^\\w\\d]+|[^\\w\\d]+$', '', k)\n",
    "    return k\n",
    "\n",
    "Cleaned = [CleanKeyword(k) for k in Keywords if CleanKeyword(k)]\n",
    "Seen = set()\n",
    "Keywords = []\n",
    "for k in Cleaned:\n",
    "    key = k.lower()\n",
    "    if key not in Seen:\n",
    "        Seen.add(key)\n",
    "        Keywords.append(k)\n",
    "\n",
    "print(\"Cleaned Keywords:\", Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd802f-df83-47fc-b93d-57adcde8e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "Freqs = {}\n",
    "N = len(Keywords)\n",
    "for i, k in enumerate(Keywords):\n",
    "    Freqs[k] = N - i  # higher weight for earlier items\n",
    "\n",
    "PossibleFonts = [\n",
    "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",         \n",
    "    \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\",\n",
    "    \"/Library/Fonts/Arial.ttf\",                               \n",
    "    \"C:\\\\Windows\\\\Fonts\\\\Arial.ttf\"                           \n",
    "]\n",
    "FontPath = None\n",
    "for p in PossibleFonts:\n",
    "    if os.path.exists(p):\n",
    "        FontPath = p\n",
    "        break\n",
    "\n",
    "WcKwargs = dict(width=800, height=400, background_color=\"white\", collocations=False)\n",
    "if FontPath:\n",
    "    WcKwargs['font_path'] = FontPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60faef-4359-4fa5-9297-fa3c75941506",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wc = WordCloud(**WcKwargs).generate_from_frequencies(Freqs)\n",
    "\n",
    "OutputFile = \"WordcloudDigitalWellness.png\"\n",
    "Wc.to_file(OutputFile)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(Wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud (Digital Wellness Keywords)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Word cloud saved to: {os.path.abspath(OutputFile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb10c6-d99f-48ac-9492-cd956bff7d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}