{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929aa8b7",
   "metadata": {},
   "source": [
    "# Advanced Python Programming (APP)\n",
    "## Course Code (CC) - CSI - 3007\n",
    "### Laboratory (Lab)\n",
    "#### Digital Assignment - 01\n",
    "\n",
    "## Mathematical Operations and Unicode Processing Lab\n",
    "### Custom User Own Client Customer Individual Synthetic Dataset  \n",
    "#### Python Programming Coding Scripting Computer Science Language\n",
    "#### Jupyter Notebook Core Main Crux Framework Library Module Binary Package  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130e8bb",
   "metadata": {},
   "source": [
    "# Advanced Python Programming - Mathematical Operations and Unicode Processing Lab\n",
    "## Student: Dharshan Raj P A\n",
    "## Register Roll Number ID : 22MIC0073\n",
    "## Date: 25-07-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1aaadd1a-43e1-4a93-ae9c-6ed84b2da369",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100 factorial calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9de343a3-6543-45da-bee0-d44db8679b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933262154439441526816992388562667004907159682643816214685929638952175999932299156089414639761565182862536979208272237582511852109168640000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "factorial_input=100;\n",
    "factorial_result=1;\n",
    "for i in range( 1,100):\n",
    "    factorial_result *=i;\n",
    "print(factorial_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c99a5629-6dc5-4d48-a1cb-02d36c199c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strings are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2249a60f-72cf-4b5e-a23e-61f3afe64b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140719343682440\n",
      "140719343682472\n",
      "140719343682504\n",
      "140719343682536\n",
      "140719343682568\n",
      "140719343682600\n",
      "140719343682632\n",
      "140719343682664\n",
      "140719343682696\n",
      "140719343682728\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "string_var=\" \";\n",
    "for i in range(10):\n",
    "    string_var =i;\n",
    "    print(id(string_var))\n",
    "print(string_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4a6c80d-e9a2-45b4-b535-7030370b535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display of characters not in keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05e7f41c-9380-400a-957f-6b404d750b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î±\n"
     ]
    }
   ],
   "source": [
    "print('\\u03B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cb38a8b-a28c-467a-97b9-6752c8584084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ó’\n"
     ]
    }
   ],
   "source": [
    "print(chr(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70269651-ae36-4c9d-9b1a-ff5e50efc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€ Î©\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "greek_letter = unicodedata.lookup('GREEK CAPITAL LETTER OMEGA')\n",
    "emoji_face = unicodedata.lookup('GRINNING FACE')\n",
    "print(emoji_face, greek_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03bbfd7e-9cc0-4c2c-9389-44f29ab3c1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(chr(97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f0929b0-9840-43be-aab0-38194daf3bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "input_char = input('enter a character:')\n",
    "print( ord(input_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61a4a203-56a6-43fb-b92e-2f6c0c270ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display of keyboard characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5cf47d3-4971-4847-83b2-189d566fa05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Unicode escape sequence for 'd' is: \\u0064\n"
     ]
    }
   ],
   "source": [
    "def to_unicode_escape_sequence(char):\n",
    "    code_point = ord(char) \n",
    "\n",
    "    if code_point <= 0xFFFF:\n",
    "        return f\"\\\\u{code_point:04x}\" \n",
    "    else:\n",
    "        return f\"\\\\U{code_point:08x}\" \n",
    "\n",
    "char_input = input('enter a character:')\n",
    "escape_sequence = to_unicode_escape_sequence(char_input)\n",
    "print(f\"The Unicode escape sequence for '{char_input}' is: {escape_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98208a0f-85c6-4726-b6e2-21ac02079078",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display of chinese character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "963bfdcf-c646-4a1b-8c75-874b7afee296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Unicode escape sequence for 'ä½ ' is: \\u4f60\n"
     ]
    }
   ],
   "source": [
    "def to_unicode_escape_sequence(char): \n",
    "    code_point = ord(char)\n",
    "    if code_point <= 0xFFFF:\n",
    "        return f\"\\\\u{code_point:04x}\"\n",
    "    else:\n",
    "        return f\"\\\\U{code_point:08x}\" \n",
    "    \n",
    "chinese_char = \"ä½ \" \n",
    "escape_sequence = to_unicode_escape_sequence(chinese_char) \n",
    "print(f\"The Unicode escape sequence for '{chinese_char}' is: {escape_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2c3d4e5-f6g7-8901-bcde-f23456789012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (from nltk) (2025.7.33)\n",
      "Requirement already satisfied: tqdm in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\dharshan raj p a\\college\\laboratory\\advanced_python_programming_laboratory\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3d4e5f6-g7h8-9012-cdef-345678901234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data with proper error handling\n",
    "print(\"Downloading NLTK data...\")\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    print(\"NLTK data downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")\n",
    "    print(\"Continuing with basic text processing...\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4e5f6g7-h8i9-0123-defg-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of speech Tagging with error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5f6g7h8-i9j0-1234-efgh-567890123456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:\n",
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\n",
      "\n",
      "==================================================\n",
      "NLTK tokenization failed: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\dhars/nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\dhars\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Using basic text processing instead...\n",
      "Sentences (using basic split):\n",
      "1. Lorem Ipsum is simply dummy text of the printing and typesetting industry\n",
      "   Words: ['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry']\n",
      "\n",
      "2. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\n",
      "   Words: ['Lorem', 'Ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s,', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book']\n",
      "\n",
      "3. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged\n",
      "   Words: ['It', 'has', 'survived', 'not', 'only', 'five', 'centuries,', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting,', 'remaining', 'essentially', 'unchanged']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text=\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\"\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Try NLTK tokenization if available\n",
    "try:\n",
    "    sentences = nltk.sent_tokenize(sample_text)\n",
    "    print(\"Sentences (using NLTK):\")\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        print(f\"{i}. {sentence}\")\n",
    "        \n",
    "        # Word tokenization\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        print(f\"   Words: {tagged_words}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"NLTK tokenization failed: {e}\")\n",
    "    print(\"Using basic text processing instead...\")\n",
    "    \n",
    "    # Basic sentence splitting\n",
    "    simple_sentences = sample_text.split('.')\n",
    "    print(\"Sentences (using basic split):\")\n",
    "    for i, sentence in enumerate(simple_sentences, 1):\n",
    "        if sentence.strip():\n",
    "            print(f\"{i}. {sentence.strip()}\")\n",
    "            \n",
    "            # Basic word splitting\n",
    "            words = sentence.split()\n",
    "            print(f\"   Words: {words}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6g7h8i9-j0k1-2345-fghi-678901234567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parts of Speech tagging for a novel from the source Gutenburg Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "g7h8i9j0-k1l2-3456-ghij-789012345678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading novel from Gutenberg...\n",
      "Novel text length: 1037253 characters\n",
      "NLTK processing failed: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\dhars/nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Dharshan Raj P A\\\\College\\\\Laboratory\\\\Advanced_Python_Programming_Laboratory\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\dhars\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Using basic text processing...\n",
      "\n",
      "First 5 sentences (basic split):\n",
      "Sentence 1: ['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Attic', 'theatre', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever']\n",
      "Sentence 2: ['You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www']\n",
      "Sentence 3: ['gutenberg']\n",
      "Sentence 4: ['org']\n",
      "Sentence 5: ['If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States,', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "novel_url = \"https://www.gutenberg.org/cache/epub/76555/pg76555.txt\"\n",
    "\n",
    "try:\n",
    "    print(\"Downloading novel from Gutenberg...\")\n",
    "    url_response = request.urlopen(novel_url)\n",
    "    raw_novel = url_response.read().decode('utf-8')\n",
    "    \n",
    "    start_of_book = \"*** START OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "    end_of_book = \"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "\n",
    "    start_index = raw_novel.find(start_of_book)\n",
    "    end_index = raw_novel.find(end_of_book)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        novel_text = raw_novel[start_index + len(start_of_book):end_index].strip()\n",
    "    elif start_index != -1: \n",
    "        novel_text = raw_novel[start_index + len(start_of_book):].strip()\n",
    "    else:  \n",
    "        novel_text = raw_novel\n",
    "\n",
    "    print(f\"Novel text length: {len(novel_text)} characters\")\n",
    "    \n",
    "    # Try NLTK processing\n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(novel_text)\n",
    "        print(\"\\nPOS Tags for the first 5 sentences (using NLTK):\")\n",
    "        for i, sentence in enumerate(sentences[:5]):\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            print(f\"Sentence {i+1}: {tagged_words}\")\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK processing failed: {e}\")\n",
    "        print(\"Using basic text processing...\")\n",
    "        \n",
    "        # Basic sentence splitting\n",
    "        simple_sentences = novel_text.split('.')\n",
    "        print(\"\\nFirst 5 sentences (basic split):\")\n",
    "        for i, sentence in enumerate(simple_sentences[:5]):\n",
    "            if sentence.strip():\n",
    "                words = sentence.split()\n",
    "                print(f\"Sentence {i+1}: {words}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading novel: {e}\")\n",
    "    print(\"Using sample text instead...\")\n",
    "    \n",
    "    # Fallback sample text\n",
    "    sample_novel_text = \"The Project Gutenberg eBook of Sample Text. This is a sample text for demonstration purposes. It contains multiple sentences for text processing analysis. We will use this text to demonstrate various text processing techniques. The text includes different types of words and punctuation.\"\n",
    "    \n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(sample_novel_text)\n",
    "        print(\"\\nPOS Tags for sample text (using NLTK):\")\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            print(f\"Sentence {i+1}: {tagged_words}\")\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK processing failed: {e}\")\n",
    "        print(\"Using basic text processing...\")\n",
    "        \n",
    "        simple_sentences = sample_novel_text.split('.')\n",
    "        print(\"\\nSample text sentences (basic split):\")\n",
    "        for i, sentence in enumerate(simple_sentences):\n",
    "            if sentence.strip():\n",
    "                words = sentence.split()\n",
    "                print(f\"Sentence {i+1}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "h8i9j0k1-l2m3-4567-hijk-890123456789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Analysis Results:\n",
      "Word count: 25\n",
      "Character count: 152\n",
      "Sentence count: 2\n",
      "Top 5 most common words: [('the', 3), ('lorem', 2), ('ipsum', 2), ('dummy', 2), ('text', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Additional text processing examples without NLTK\n",
    "import re\n",
    "\n",
    "def basic_text_analysis(text):\n",
    "    \"\"\"Basic text analysis without NLTK\"\"\"\n",
    "    # Word count\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # Character count\n",
    "    char_count = len(text)\n",
    "    \n",
    "    # Sentence count (basic)\n",
    "    sentences = text.split('.')\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    \n",
    "    # Most common words\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word = word.lower().strip('.,!?;:()\"\\'')\n",
    "        if word:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    # Top 5 most common words\n",
    "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'top_words': top_words\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s.\"\n",
    "analysis = basic_text_analysis(test_text)\n",
    "\n",
    "print(\"Text Analysis Results:\")\n",
    "print(f\"Word count: {analysis['word_count']}\")\n",
    "print(f\"Character count: {analysis['char_count']}\")\n",
    "print(f\"Sentence count: {analysis['sentence_count']}\")\n",
    "print(f\"Top 5 most common words: {analysis['top_words']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2-m3n4-5678-ijkl-901234567890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
